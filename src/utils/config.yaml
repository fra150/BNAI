# BNAI Configuration File

model:
  latent_dim: 128
  output_dim: 21  # Maintaining existing parameter count
  hidden_layers: [256, 512]
  activation: "relu"
  interpretability:
    enable_xai: true
    saliency_maps: true
    attention_analysis: true

training:
  epochs: 100
  batch_size: 32
  learning_rate: 0.001

data:
  save_path: "models/bnai_model.pth"
  data_path: "src/data/synthetic_bnai_data.json"
  save_path: "models/bnai_clone.pth"
  train_data_path: "data/train"
  val_data_path: "data/val"
  test_data_path: "data/test"
  preprocessing:
    normalize: true
    augmentation: false
    cache_dir: "cache"

# Keeping all existing bnai_params (A through Z) as they are
bnai_params:
  - name: 'A'  # Adaptability
    type: 'float'
    min_val: 0.0
    max_val: 2.0
    default: 1.2
  - name: 'E_g'  # Evolution
    type: 'float'
    min_val: 0.0
    max_val: 2.0
    default: 1.1
  - name: 'G'  # Growth
    type: 'float'
    min_val: 0.0
    max_val: 3.0
    default: 2.5
  - name: 'H'  # Learning Entropy
    type: 'float'
    min_val: 0.0
    max_val: 2.0
    default: 1.3
  - name: 'I'  # Interconnection
    type: 'float'
    min_val: 0.0
    max_val: 1.0
    default: 0.8
  - name: 'L'  # Learning Level
    type: 'float'
    min_val: 0.0
    max_val: 2.0
    default: 1.5
  - name: 'P'  # Processing Power
    type: 'float'
    min_val: 0.0
    max_val: 2.0
    default: 1.0
  - name: 'Q'  # Quality
    type: 'float'
    min_val: 0.0
    max_val: 3.0
    default: 3.0
  - name: 'R'  # Robustness
    type: 'float'
    min_val: 0.0
    max_val: 1.0
    default: 0.95
  - name: 'U'  # Autonomy
    type: 'float'
    min_val: 0.0
    max_val: 2.0
    default: 1.1
  - name: 'V'  # Speed
    type: 'float'
    min_val: 0.0
    max_val: 1.0
    default: 0.9
  - name: 'O'  # Optimization
    type: 'float'
    min_val: 0.0
    max_val: 1.0
    default: 1.0
  - name: 'T'  # Time Factor
    type: 'float'
    min_val: 0.0
    max_val: 2.0
    default: 1.2
  - name: 'S'  # Stability
    type: 'float'
    min_val: 0.0
    max_val: 1.0
    default: 1.0
  - name: 'F'  # Flexibility
    type: 'float'
    min_val: 0.0
    max_val: 1.0
    default: 0.85
  - name: 'B'  # Bias
    type: 'float'
    min_val: 0.0
    max_val: 1.0
    default: 0.5
  - name: 'C'  # Complexity
    type: 'float'
    min_val: 0.0
    max_val: 1.0
    default: 0.3
  - name: 'E'  # Error Rate
    type: 'float'
    min_val: 0.0
    max_val: 1.0
    default: 0.2
  - name: 'M'  # Memory
    type: 'float'
    min_val: 0.0
    max_val: 10.0
    default: 5.0
  - name: 't'  # Time
    type: 'float'
    min_val: 0.0
    max_val: 5.0
    default: 2.0
  - name: 'D'  # Durability (Durabilit√†)
    type: 'float'
    min_val: 0.0
    max_val: 1.0
    default: 0.75

  - name: 'K'  # Knowledge Retention (Ritenzione della Conoscenza)
    type: 'float'
    min_val: 0.0
    max_val: 1.0
    default: 0.85

  - name: 'Z'  # Self-optimization (Auto-ottimizzazione)
    type: 'float'
    min_val: 0.0
    max_val: 1.0
    default: 0.70

# Calculated values based on the formulas
# Adding new configuration sections
weights:
  parameter_weights:  # w_i weights from formula
    A: 0.1
    E_g: 0.1
    G: 0.08
    H: 0.07
    I: 0.07
    L: 0.08
    P: 0.06
    Q: 0.09
    R: 0.08
    U: 0.07
    V: 0.06
    O: 0.07
  alpha: 0.5  # Balancing coefficient
  beta: 0.1   # Balancing coefficient

security:
  ood_detection: true
  prompt_injection_resistance: true
  adversarial_detection: true
  privacy_compliance: true

metrics:
  calibration:
    use_ece: true
    ece_threshold: 0.1
  task_specific:
    enable_bleu: true
    enable_rouge: true
    enable_f1: true
    enable_em: true

formula:
  normalization:
    use_sigmoid: true
    sigmoid_scale: 2.0
  regularization:
    type: "l2"
    alpha: 0.01
    lambda_reg_learning: true
  decay:
    use_time_decay: true
    memory_factor: true

validation:
  test_transfer_learning: true
  test_meta_learning: true
  test_regularization: true
  normalization_check: true

# Keeping existing calculated values
calculated_values:
  exp_term: 0.6703200460356393
  numerator: 9.92574
  denominator: 1.6703200460356393

regularization_type: "l2"
regularization_alpha: 0.01